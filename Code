# -*- coding: utf-8 -*-
"""MLHELL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jIgyJxm6-uYO-dCpEzKIpWPjoS28JiA6
"""

from google.colab import drive
drive.mount('/content/drive')

"""Data processing"""

# def load_data(directory, target_size, train_true ,csv_path):
#     images = []
#     key = []
#     labels = []
#     label_final= []
#     skips = []
#     count = 0
#     count_var =0
#     count_valid =0
#     if train_true:
#       key = create_key('category (2).csv')
#       csv_file_path = csv_path

#       with open(csv_file_path, newline='') as csvfile:
#         reader = csv.reader(csvfile)
#         next(reader, None)
#         for row in reader:
#           labels.append(key.index(row[2]))

# ###############################################################################
#     with open("labels_train_small_new.csv", 'w', newline='') as csv_file:
#         writer = csv.writer(csv_file)
#         count_valid = 1
#         dir=os.listdir(directory)
#         dir.sort(key=lambda x: int(re.search(r'\d+', x).group()))
#         face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default (1).xml')
#         for image_file in dir:
#             if(count % 1000 == 0):
#                 print("Loaded", count)
#             if(count % 5000 == 0 and count_valid):
#                 if not os.path.exists(f"im_proc0"):
#                        os.makedirs(f"im_proc0")
#                        print(f"im_proc0")
#                 count_var += 1

#             image_path = os.path.join(directory, image_file)
#             number = re.findall(r'\d+', image_path)[0]
#             number = int(number)
#             img = cv2.imread(image_path)
#             if (img is not None) :
#                 count_valid = 1
#                 if(train_true):
#                     writer.writerow([count, labels[number] , key[labels[number]]])
#                     label_final.append(labels[number])
#                 gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#                 faces = face_cascade.detectMultiScale(gray, 1.05, 11)

#                 if len(faces) > 0:
#                      largest_face_area = 0
#                      largest_face = None
#                      for (x, y, w, h) in faces:
#                          if w * h > largest_face_area:
#                              largest_face_area = w * h
#                              largest_face = (x, y, w, h)

#                      if largest_face is not None:
#                          x, y, w, h = largest_face
#                          img_cropped = img[y:y + h, x:x + w]
#                          resized_img = cv2.resize(img_cropped, target_size)
#                      else:
#                          resized_img = cv2.resize(img, target_size)

#                      f_name = f"im_proc0/{count}.jpg"
#                      cv2.imwrite(f_name, resized_img)
#                      images.append(resized_img)
#                      print(f"IM write {f_name}")
#                 else:
#                     resized_img = cv2.resize(img, target_size)
#                     f_name = f"im_proc0/{count}.jpg"
#                     cv2.imwrite(f_name, resized_img)
#                     images.append(resized_img)
#                     print(f"IM write {f_name}")
#                 count = count + 1
#             else:
#               count_valid = 0
#               skips.append(number)

#         print("Size of directory:", len(dir))
#         print("Number of images loaded:", len(images))
#         #print("Number of images loaded:", len(eval_images))
#         print("Number of labels:",len(label_final))

#     return np.array(images) , key , label_final, skips



# train_images, key, train_labels, _= load_data("train_small/train_small",target_size=(128,128), train_true=1,csv_path="train_small (2).csv")
# #load_data("train/train",target_size=(256,256), train_true=1,csv_path="train.csv")
# #val_images, _ , val_labels, _ = load_data("train_small/train_small",target_size=(256,256), train_true=1,csv_path="train_small.csv")

"""Code ran on my machince to do image preprocessing, used but cahnged parameters for the face detection
https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81
"""

import cv2
import csv
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import re
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Conv2D, MaxPooling2D, GlobalAveragePooling2D , BatchNormalization ,  LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.efficientnet import EfficientNetB0
import math
import gc

def create_key(csv_file_path):

  key = []
  with open(csv_file_path, newline='') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
      key.append(row[1])

  return key[1:]

def split(train_images,train_lables ):
  split = int(len(train_images) * 0.8)
  X_train = train_images[0:split]
  y_train = train_lables[0:split]
  y_train  = tf.keras.utils.to_categorical(y_train , num_classes=100)
  X_val = train_images[split:]
  y_val = np.array(train_lables[split:])
  y_val  = tf.keras.utils.to_categorical(y_val , num_classes=100)
  return X_train, y_train, X_val, y_val

early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,restore_best_weights=True,start_from_epoch=20)


def create_model2(num_classes):
    base_model = MobileNet0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    x = Dense(4000, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(4000, activation='relu')(x)
    x = Dropout(0.5)(x)
    # x = Dense(256, activation='relu')(x)
    # x = Dropout(0.5)(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    #https://afrozchakure.medium.com/hyper-parameter-tuning-to-optimize-dl-models-mobilenet-v2-511463aae764
    base_model.trainable = True
    fine_tune_at = 100
    for layer in base_model.layers[:fine_tune_at]:
      layer.trainable = False

    model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

mod3 = create_model2(num_classes=100)


key = create_key('/content/drive/MyDrive/Colab Notebooks/category.csv')
train_labels_full = []
csv_file_path = '/content/drive/MyDrive/Colab Notebooks/Train_again/labels_train_new(1).csv'
with open(csv_file_path, newline='') as csvfile:
  reader = csv.reader(csvfile)
  for row in reader:
    train_labels_full.append(int(row[1]))



dirty = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]
#need 15
playbakc = 0
train_lables_max=[]
train_images_full = []
train_images=[]
for i in dirty:
  train_lables=[]

  print(i)
  dir=os.listdir(f'/content/drive/MyDrive/Colab Notebooks/Train_again/im_proc_train{i}')
  dir.sort(key=lambda x: int(re.search(r'\d+', x).group()))
  for filei in dir:
    image_path = os.path.join(f'/content/drive/MyDrive/Colab Notebooks/Train_again/im_proc_train{i}', filei)
    img = cv2.imread(image_path)
    img= cv2.resize(img,(128,128))
    train_images.append(img)

  print(len(train_images))


  # train_images = train_images.squeeze()
  # train_images = train_images.astype("float32") / 255

  # if i < 4:
  #   train_images_full.append(train_images)
  #   train_lables_max.append(train_lables)

  # X_train , y_train, X_val , y_val = split(train_images, train_lables)

  print(f"Training on dataset {i}")


  # Train the model on the current dataset
  # if i % 4 == 0:
  #   history = mod3.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))
  #   X_train , y_train, X_val , y_val = split(train_images_full[playbakc], train_lables_max[playbakc])
  #   del X_train
  #   gc.collect()
  #   history = mod3.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))
  #   playbakc = playbakc + 1
  # else:
  if i in [14]:
    if i == 14:
      train_lables = train_labels_full[(i-14)*5000:((i-1)*5000) + 3215]
      print((i-14)*5000,((i-1)*5000) + 3215)
    else:
      train_lables = train_labels_full[(i-14)*5000: (i) *5000 ]
      print((i-14)*5000,(i) *5000 )
      print(len(train_lables))
    train_images = np.array(train_images)
    train_images = train_images.squeeze()
    train_images = train_images.astype("float32") / 255
    X_train , y_train, X_val , y_val = split(train_images, train_lables)
    history = mod3.fit(X_train, y_train, epochs=40, batch_size=8, validation_data=(X_val, y_val), callbacks=[early_stopping_callback])
    train_images= []
    del X_train
    gc.collect()

mod3.save('/content/drive/MyDrive/Colab Notebooks/my_model(14).keras')

import cv2
import csv
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import re
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Conv2D, MaxPooling2D, GlobalAveragePooling2D , BatchNormalization ,  LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.applications import ResNet50
import math
import gc
val_images = []
modele_loaded = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/my_model(12).keras')
dir=os.listdir(f'/content/drive/MyDrive/Colab Notebooks/im_proc_test')
dir.sort(key=lambda x: int(re.search(r'\d+', x).group()))
for filei in dir:
  image_path = os.path.join(f'/content/drive/MyDrive/Colab Notebooks/im_proc_test', filei)
  img = cv2.imread(image_path)
  img= cv2.resize(img,(224,224))
  val_images.append(img)

val_images = np.array(val_images)
print(len(val_images))

val_labels = []
csv_file_path = '/content/drive/MyDrive/Colab Notebooks/labels_test_new.csv'
with open(csv_file_path, newline='') as csvfile:
  reader = csv.reader(csvfile)
  for row in reader:
    val_labels.append(int(row[1]))

train_images = val_images.squeeze()
X_test = val_images.astype("float32") / 255
y_test = np.array(val_labels)
y_test  = tf.keras.utils.to_categorical(y_test , num_classes=100)

test_loss, test_acc = modele_loaded.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc * 100}')

import cv2
import csv
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import re
import os
import numpy as np
from tensorflow.keras.preprocessing.image import load_img, img_to_array

import imageio.v3 as imageio

def create_key(csv_file_path):

  key = []
  with open(csv_file_path, newline='') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
      key.append(row[1])

  return key[1:]



def load_data(directory, target_size, train_true ,csv_path):
    images = []
    key = []
    labels = []
    label_final= []
    skips = []
    count = 0

    key = create_key('/content/drive/MyDrive/Colab Notebooks/category.csv')

    if train_true:
      csv_file_path = csv_path
      with open(csv_file_path, newline='') as csvfile:
        reader = csv.reader(csvfile)
        next(reader, None)
        for row in reader:
          labels.append(key.index(row[2]))

###############################################################################


    dir=os.listdir(directory)
    dir.sort(key=lambda x: int(re.search(r'\d+', x).group()))
    face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Colab Notebooks/haarcascade_frontalface_default.xml')
    for image_file in dir:
        if(count % 1000 == 0):
            print("Loaded", count)
        count = count + 1
        image_path = os.path.join(directory, image_file)
        number = re.findall(r'\d+', image_file)[0]
        number = int(number)
        # Read the input image
        img = cv2.imread(image_path)
        if (img is not None) :
            if(train_true):
                label_final.append(labels[number])

            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.05, 11)
            if len(faces) > 0:

                 x, y, w, h = faces[0]
                 img = img[y:y+(h), x:x+(w)]
                 resized_img = cv2.resize(img, target_size)
                 images.append(resized_img)
            else:
                resized_img = cv2.resize(img, target_size)
                images.append(resized_img)
        else:
          skips.append(number)
    print("Size of directory:", len(dir))
    print("Number of images loaded:", len(images))
    #print("Number of images loaded:", len(eval_images))
    print("Number of labels:",len(label_final))

    return np.array(images) , key , label_final, skips






#train_images, key, train_labels, _ = load_data("/content/drive/MyDrive/Colab Notebooks/train",target_size=(256,256), train_true=1,csv_path="/content/drive/MyDrive/Colab Notebooks/train.csv")
#val_images,key , val_labels,_ = load_data("/content/drive/MyDrive/Colab Notebooks/train_small",target_size=(256,256), train_true=1,csv_path="/content/drive/MyDrive/Colab Notebooks/train_small.csv")
test_images, key,_ ,skips = load_data("/content/drive/MyDrive/Colab Notebooks/test (1)/test", target_size=(224,224),train_true=0,csv_path="/content/drive/MyDrive/Colab Notebooks/train_small.csv")




#eval_images, key, eval_labels = load_data("/content/drive/MyDrive/train_small")

"""Didn't precporssee the final images did that in collab using the code above but see first block

"""

test_images= test_images.squeeze()
X_out = test_images.astype("float32") / 255

preds = modele_loaded.predict(X_out)

preds = preds.argmax(axis=1)
preds=list(preds)

def write_out(preds,key,skips):
 print(len(preds))
 print(len(skips))
 print(skips)
 with open('/content/drive/MyDrive/Colab Notebooks/sub(2).csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        j = 0
        writer.writerow(['Id', 'Category'])
        for i in range(4977):
            if i in skips:
                writer.writerow([i, key[preds[i%23]]])
            else:
                #print(j)
                writer.writerow([i, key[preds[j]]])
                j = j + 1

print(preds)
write_out(preds,key,skips)

